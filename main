#!/usr/bin/env python
from __future__ import division
import os
import sys
sys.path.insert(0,'/extra/adarsh/clusterpheno')
from clusterpheno.helpers import *
from myProcesses import Hc_HW_tautau_ll_14_TeV_collection
from tqdm import tqdm
from ConfigParser import SafeConfigParser
import argparse
import numpy as np
import pandas as pd
import shutil as sh
import subprocess as sp
import glob
import gzip
from BDTClassifier import BDTClassifier

import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
figwidth = 3.06
plt.rcParams['figure.figsize'] = (figwidth,figwidth)
import itertools as it
import untangle

# Procedure
# - Create directories
# - Modify param_card
# - Copy delphes and run cards
# - Write submit scripts
# - Submit jobs to cluster

def set_axis_labels():
    # plt.xlim(525,2000)
    # plt.ylim(0,1500)
    # plt.text(600, 1300, r"$\mathcal{L} = 3000$ $\mathrm{fb}^{-1}$", fontsize = 20)
    # plt.text(700, 870, r"$M_1 = |\mu|$", fontsize = 11, rotation = 32)
    plt.ylabel(r'$m_{H}$', fontsize = 11)
    plt.xlabel(r'$m_{H}^\pm = m_A$', fontsize = 11)
    axes = plt.axes()
    # axes.xaxis.set_label_coords(0.5, -0.1)
    # axes.yaxis.set_label_coords(-0.08, 0.5)
    # plt.tight_layout()

def get_XYZ_grid(df,res):
    df = df.replace('None',0)
    x = df['mC']
    y = df['mH']
    z = df['Significance']

    resX = res; resY = res
    xi = np.linspace(0, max(df['mH']), resX)
    yi = np.linspace(0, max(df['mC']), resY)
    X, Y = np.meshgrid(xi, yi)
    Z = matplotlib.mlab.griddata(x,y,z,xi,yi,interp='nn')
    return X,Y,Z

def get_max_bdt_significance(signal):
    with cd('MakeFeatureArrays/Output/'+signal.index+'/'):
        df = pd.read_csv('nevents.csv')
        return max(df['significance'])

def collect_max_bdt_sigs(signals):
    with open('intermediate_results/bdt_sigs.txt', 'w') as f:
        f.write('mH,mC,Significance\n')
        for signal in tqdm(signals):
            f.write(','.join([str(signal.bp['mH']),
                              str(signal.bp['mC']),
                              str(get_max_bdt_significance(signal))])+'\n')

def get_original_bg_events(bg_name):
    # filepath = 'BackgroundFeatureArrays/Output/{}/Analysis/Cutflows/Signal'.format(bg_name)
    filepath = 'MakeFeatureArrays/Output/{}/Analysis_0/Cutflows/Signal'.format(bg_name)
    return Counter((get_SAF_objects(filepath)).InitialCounter).nevents

def calculate_significance(nS,nB):
    """ Calculate significance given number of signal and background events."""
    if nS < 5: return 0. # Require a minimum number of signal events.
    else: return nS/np.sqrt(B)

def create_directories(signals,nb_cores):
    # to be done on an Ocelote node
    if nb_cores==1:
        map(lambda x: x.create_directory('/extra/adarsh/Tools/mg5'),
            tqdm(signals, desc='Creating MG5 directories', ncols=80))
    else:
        do_parallel(lambda x: x.create_directory('/extra/adarsh/Tools/mg5'),
                    signals,nb_cores)
    map(lambda x: x.set_parameters(), tqdm(signals, desc='Setting parameters'))

def copy_cards(signals):
    for signal in tqdm(signals):
        signal.copy_cards('Cards/run_cards/100_TeV_run_card.dat',
            'Cards/pythia_cards/pythia_card.dat','Cards/delphes_cards/FCChh.tcl')
        sh.copy('Cards/delphes_cards/momentumResolutionVsP.tcl', signal.directory+'/Cards/')
        sh.copy('Cards/delphes_cards/muonMomentumResolutionVsP.tcl', signal.directory+'/Cards/')
        # sh.copy('Cards/me5_configuration.txt', signal.directory+'/Cards/')

def write_pbs_scripts(processes, parser, nruns):
    for process in tqdm(processes, dynamic_ncols = True,
                        desc = 'Writing PBS submit scripts'):
        with open(parser.get('PBS Templates', 'generate_script'), 'r') as f:
            string = f.read()
        with open(process.directory+'/generate_events.pbs', 'w') as f:
            f.write(string.format(jobname =process.index,
            username = parser.get('Cluster', 'username'),
            email = parser.get('Cluster', 'email'),
            group_list = parser.get('Cluster', 'group_list'),
            nruns = str(nruns),cwd = os.getcwd(),
            cput = str(5*nruns),walltime = str(1*nruns), # in hrs
            mg5_process_dir = process.directory))

def rename_folder(signal):
    with cd('Events/Signals/'+signal.name+'/'+signal.decay_channel+'/'+signal.energy):
        sh.move(signal.index,'mH_'+str(int(float(signal.bp['mH'])))+'_mC_'+str(int(float(signal.bp['mC']))))

def make_signal_feature_arrays(signals):
    for signal in tqdm(signals):
        scriptName = signal.index+'.pbs'
        with open(scriptName,'w') as f:
            f.write('''\
#!/bin/bash
#PBS -M adarsh@email.arizona.edu
#PBS -W group_list=shufang
#PBS -N {proc_name}
#PBS -q windfall
#PBS -l select=1:ncpus=1:mem=6gb:pcmem=6gb
#PBS -l place=free:shared
#PBS -l cput=5:0:0
#PBS -l walltime=5:0:0
date
cd /xdisk/adarsh/ExoticHiggs/MakeFeatureArrays/Build
./analyze.sh {proc_name}
date
exit 0
'''.format(proc_name = signal.index))
        sp.call(['qsub',scriptName],stdout=open(os.devnull,'w'))
        os.remove(scriptName)

def make_bg_feature_array(bg_name,bg_decay_channel):
    with cd('MakeFeatureArrays/Build'):
        sp.call(['./analyze_bgs.sh',bg_name,bg_decay_channel])

def data_cleaning(proc_name):
    header = 'ptl1,ptl2,ptb1,ptj1,ptj2,MET,THT\n'
    with open('MakeFeatureArrays/Output/'+proc_name+'/feature_array.txt','r+') as f:
        lines = f.readlines()
        if not lines[0].startswith('p'):
            f.seek(0)
            lines.insert(0,header)
            f.writelines(lines)
            f.truncate()

def make_bdt_cut_flow_table(BDTClassifier, bdt_cut, write_table=False):
    df = pd.DataFrame(index  = ['After preselection',
                                'After BDT cut'])

    bgs = ['tt_bbllvv','tt_semileptonic']
    decisions = {}
    for process in ['Signal']+bgs:
        decisions[process] = BDTClassifier.clf.decision_function(BDTClassifier.test_sets[process])

    def feature_array_length(process_name):
        return len(BDTClassifier.train_sets[process_name]+BDTClassifier.test_sets[process_name])

    for process_name in ['Signal']+bgs:
        df[process_name] = [len(decisions[process_name]),
                            len(filter(lambda x: x > bdt_cut,
                            decisions[process_name]))]
    signal_filepath = 'MakeFeatureArrays/Output/'+BDTClassifier.signal.index+'/Analysis_0/Cutflows/Signal'
    original_signal_events = Counter((get_SAF_objects(signal_filepath)).InitialCounter).nevents

    def fraction_after_preselection(process_name):
        if process_name == 'Signal':
            return feature_array_length(process_name)/original_signal_events
        else:
            return feature_array_length(process_name)/get_original_bg_events(process_name)

    signal_xsection = BDTClassifier.signal.get_xsection()*fraction_after_preselection('Signal')

    df['Signal_xs'] = (df['Signal']/df['Signal'][0])*signal_xsection

    xsections = {}
    for bg in bgs:
        with open('/extra/adarsh/Events/Backgrounds/'+bg.split('_')[0]+'/'+bg.split('_')[1]+'/100_TeV/mean_xsection.txt','r') as f:
            xsections[bg] = float(f.readlines()[0][0])
            xsections[bg] = xsections[bg]*fraction_after_preselection(bg)

    for bg in bgs:
        df[bg+'_xs'] = ((df[bg]+3)/df[bg][0])*xsections[bg]

    df['bg_xs_total'] = sum([df[bg+'_xs'] for bg in bgs])

    luminosity = 3000.

    for process_name in ['Signal']+bgs:
        df[process_name+'_events'] = df[process_name+'_xs']*luminosity

    df['bg_events_tot'] = sum([df[bg+'_events'] for bg in bgs])
    df['$S/B$'] = df['Signal_events']/df['bg_events_tot']
    df['Significance'] = df['Signal_events']/np.sqrt(df['bg_events_tot'])

    if write_table == True:
        with open('tables/paper_bdt_cutflowtable.tex', 'w') as f:
            df_paper = df[['Signal_xs']+[bg+'_xs' for bg in bgs]+['bg_xs_total','$S/B$','Significance']]
            S = BDTClassifier.signal.get_xsection()
            B = sum([xsections[bg] for bg in bgs])
            df_paper.columns = ['$\sigma_{signal}$','$\sigma_{tt}(f)$','$\sigma_{tt}(s)$','$\sigma_{tot,BG}$','$S/B$','$S/\sqrt{B}$']
            df_paper.loc['Original'] = [S]+[xsections[bg] for bg in bgs]+[B,S/B,S*np.sqrt(luminosity/B)]
            df_paper.reindex(['Original','After preselection','After BDT cut'])
            def myFormatter(x):
                if x < 0.001: return '%.1e' % x
                if x < 0.01: return '%.3f' % x
                if x < 1: return '%.2f' % x
                if x < 10: return '%.1f' % x
                else: return '{:,.0f}'.format(x)
            df_paper.to_latex(f,escape=False,float_format = myFormatter)
    return df

processes = ['Signal', 'tt_bbllvv', 'tt_semileptonic']
labels = {
'Signal':'mA_300_tb_1',
'tt_bbllvv':'tt_bbllvv',
'tt_semileptonic':'tt_semileptonic',
}

colors = {
          'Signal':'DarkBlue',
            'tt_bbllvv':'r',
            'tt_semileptonic':'green',
          }
patches = {}
for process in processes:
    patches[process] = mpatches.Rectangle((1,1),0.5,0.5, color = colors[process],
                                          label = r'${}$'.format(labels[process]), alpha = 0.4)

def collect_bdt_responses(BDTClassifier):
    for process in processes:
        with open('intermediate_results/bdt_responses/'+process+'.txt', 'w') as f:
            responses = BDTClassifier.clf.decision_function(BDTClassifier.test_sets[process])
            f.write('\n'.join(map(lambda x: str(x), responses)))

def make_bdt_histo():
    matplotlib.style.use('ggplot')
    responses = {}
    for process in processes:
        with open('intermediate_results/bdt_responses/'+process+'.txt', 'r') as f:
            responses[process] = map(lambda x: float(x), f.readlines())

    def weights(array):
        return np.ones_like(array)/float(len(array))
    plt.hist(responses['Signal'], weights = weights(responses['Signal']), bins = 30,
             color = 'DarkBlue', alpha = 0.4, label = labels['Signal'])
    plt.hist(responses['tt_bbllvv'], weights = weights(responses['tt_bbllvv']), bins = 30,
             color = 'Crimson', alpha = 0.4, label = labels['tt_bbllvv'])
    plt.hist(responses['tt_semileptonic'], weights = weights(responses['tt_semileptonic']), bins = 30,
             color = 'Green', alpha = 0.4, label = labels['tt_semileptonic'])
    plt.xlim(-5, 5)
    plt.ylim(0,0.3)
    plt.ylabel(r'$\frac{1}{\sigma}\frac{d\sigma}{dx}$',fontsize = 11)
    plt.xlabel(r'$\mathrm{BDT}$ $\mathrm{Response}$',fontsize = 11)
    axes = plt.axes()
    plt.locator_params(nbins = 8)
    axes.xaxis.set_label_coords(0.5, -0.2)
    plt.legend(handles = [patches[process] for process in processes], fontsize = 11, loc = 'upper center')
    plt.tight_layout()
    plt.savefig('images/bdt_response.pdf')
    plt.close()

def get_significance(classifier, bdt_cut):
    table = make_bdt_cut_flow_table(classifier, bdt_cut)
    sig = table['Significance'][-1]
    return sig

def make_combined_contour_plot():
    bdt_df = pd.read_csv('intermediate_results/bdt_sigs.txt')

    plt.style.use('ggplot')

    res = 15
    fmt = {}
    X, Y, Z = get_XYZ_grid(bdt_df,res)
    # colors = ['DarkBlue','Maroon']
    bdt_CS = plt.contour(X,Y,Z,levels=[1.96,5],linestyles = 'dashed')
    set_axis_labels()
    x = np.arange(0,2000, 0.1) 
    plt.plot(x,x,color = 'gray', linestyle = 'dashed')


    # BDT contour labels
    # fmt[bdt_CS.levels[0]] = r'$1.96\sigma$ $(BDT)$'
    # fmt[bdt_CS.levels[1]] = r'$5\sigma$ $(BDT)$'
    # manual_locations = [(1200,800),(1500,1200)]
    plt.clabel(bdt_CS, inline=1, fontsize=11)

    # set_axis_labels()
    plt.locator_params(axis='x',nbins=6)
    plt.locator_params(axis='y',nbins=6)
    # plt.text(600, 1200, r"${}\sigma$".format(levels[0]), fontsize = 30)

    plt.tight_layout()
    filename = 'combined'
    plt.savefig('images/{}.pdf'.format(filename),dpi = 300)

def write_nevents_to_file(signal):
    """ Scan over a range of bdt cuts and write the results to a file. """
    classifier = BDTClassifier(signal)
    # with open(signal.directory+'/MakeFeatureArray/nevents.csv', 'w') as f:
    with open('MakeFeatureArrays/Output/'+signal.index+'/nevents.csv', 'w') as f:
        f.write('bdt_cut,nS,nB,significance\n')
        for bdt_cut in tqdm(np.arange(0.0,15.0,0.1)):
            table = make_bdt_cut_flow_table(classifier, bdt_cut)
            nS = table['Signal_events'][-1]
            nB = table['bg_events_tot'][-1]
            sig = table['Significance'][-1]
            f.write('{},{},{},{}\n'.format(str(bdt_cut),
                    str(int(nS)), str(int(nB)), str(sig)))

def get_bg_xsection_from_LHCO_file(filename):
    """ Get matched cross section from a .lhco.gz file"""
    with gzip.open(filename, 'r') as f:
        lines = f.readlines()
        myline = [line for line in lines if 'Matched Integrated weight' in line][0]
        bg_xsection = float(myline.split()[-1])
        return bg_xsection

def collect_bg_xsections(bg_name):
    """ Collect cross sections from multiple .lhco.gz files and average them."""
    with cd('/extra/adarsh/Events/Backgrounds/'+bg_name+'/100_TeV/'):
        filenames = glob.glob('*/Events/*/*.lhco.gz')
        mean_xsection = np.mean(map(get_bg_xsection_from_LHCO_file,
            tqdm(filenames, desc = 'getting xsections for '+bg_name, dynamic_ncols=True)))
        with open('mean_xsection.txt', 'a') as f:
            f.write(str(mean_xsection))

def collect_signal_cross_sections(signals):
    map(lambda x: x.get_xsection_from_LHCO_files(),tqdm(signals,
        desc='Collecting signal cross sections from LHCO files'))

def main():
    mg5_path = '/extra/adarsh/Tools/mg5/'
    configparser = SafeConfigParser()
    configparser.read('config.ini')

    argparser = argparse.ArgumentParser(description="""
        The ExoticHiggs project interface
        An interface for performing event generation and analysis on the cluster.
        Based on the clusterpheno package.
        Creator: Adarsh Pyarelal""")
    argparser.add_argument("--collect_xsections", action = "store_true",
                            help = "Collecting cross sections from LHCO files")
    argparser.add_argument("-cd","--create_dirs",action="store_true",
                           help = "Create MG5 directories for the process")
    argparser.add_argument("-cc","--copy_cards",
                           help = "Copying cards",action="store_true")
    argparser.add_argument("--write_pbs_scripts",action="store_true",
                           help = "Write PBS event generation scripts")
    argparser.add_argument("-sj","--submit_jobs",action="store_true",
                           help = "Submit event generation jobs to cluster")
    argparser.add_argument("-mfa","--make_feature_arrays",help = "Make signal feature arrays",action="store_true")
    argparser.add_argument("--clean_data",action="store_true",
        help = "Perform data cleaning (check if feature arrays have headers)")
    argparser.add_argument("--bdt_rep",help = "Perform BDT scan for a single benchmark point",action="store_true")
    argparser.add_argument("--bdt_scan",help = "Perform BDT scan",action="store_true")
    argparser.add_argument("--bdt_histo",help = "Make bdt response histo",action="store_true")
    argparser.add_argument("--nb_cores",help = "Specify number of cores to use",type=int,default=28)
    argparser.add_argument("--collect_bdt_sigs",help = "Collect max bdt significances",action="store_true")
    argparser.add_argument("--make_contour_plot",help = "Make contour plot",action="store_true")
    argparser.add_argument("--make_bg_feature_arrays",action="store_true",
            help="Make background feature arrays and perform data cleaning")
    argparser.add_argument("--rename",action="store_true",
            help="Rename signal folders")
    args = argparser.parse_args()

    processes = {
        "Hc_HW_tautau_ll_14_TeV":Hc_HW_tautau_ll_14_TeV_collection,
    }

    if args.rename: 
        map(rename_folder,tqdm(processes["Hc_HW_tautau_ll_14_TeV"]))
    if args.create_dirs: 
        create_directories(processes["Hc_HW_tautau_ll_14_TeV"],args.nb_cores)
    if args.copy_cards: 
        copy_cards(processes["Hc_HW_tautau_ll_14_TeV"])
    if args.write_pbs_scripts: 
        write_pbs_scripts(processes["Hc_HW_tautau_ll_14_TeV"], configparser, 10)
    if args.submit_jobs: 
        map(lambda x: x.generate_events(),
            tqdm(processes["Hc_HW_tautau_ll_14_TeV"], desc = "submitting PBS jobs"))
    if args.make_feature_arrays:
        make_signal_feature_arrays(processes["Hc_HW_tautau_ll_14_TeV"])
    if args.make_bg_feature_arrays:
        make_bg_feature_array('tt','bbllvv')
        data_cleaning('tt_bbllvv')
        make_bg_feature_array('tt','semileptonic')
        data_cleaning('tt_semileptonic')
    if args.clean_data: 
        map(data_cleaning,tqdm([signal.index for signal in processes['Hc_HW_tautau_ll_14_TeV']]))
    if args.collect_xsections: 
        collect_signal_cross_sections(processes["Hc_HW_tautau_ll_14_TeV"])
        # collect_bg_xsections('tt/semileptonic')
        # collect_bg_xsections('tt/bbllvv')
    if args.bdt_scan:
        if args.nb_cores == 1:
            map(write_nevents_to_file,tqdm(processes["Hc_HW_tautau_ll_14_TeV"]))
        else:
            do_parallel(write_nevents_to_file,processes["Hc_HW_tautau_ll_14_TeV"], args.nb_cores)
    if args.collect_bdt_sigs:
        collect_max_bdt_sigs(processes['Hc_HW_tautau_ll_14_TeV'])
    if args.make_contour_plot:
        make_combined_contour_plot()

    if args.bdt_histo: 
        myClassifier = BDTClassifier(processes['Hc_HW_tautau_ll_14_TeV'][0])
        collect_bdt_responses(myClassifier)
        make_bdt_histo()
        # bdt_cut_range = tqdm(np.arange(0.0,15.0,0.1))
    if args.bdt_rep: 
        myClassifier = BDTClassifier(processes['Hc_HW_tautau_ll_14_TeV'][0])
        # bdt_cut_range = tqdm(np.arange(0.0,15.0,0.1))
        # print(max(map(lambda x: get_significance(myClassifier, x),bdt_cut_range)))
        make_bdt_cut_flow_table(myClassifier,0)

if __name__ == '__main__':
    main()
